{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ea77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from simple_grid import simple_grid as gridworld\n",
    "from simple_grid_agent import GridworldAgent as Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4359ef54",
   "metadata": {},
   "source": [
    "Read through all the classes and functions defined inside `simple_grid` environment and `GridworldAgent` to familiarize yourself with the details of this assignment.\n",
    "\n",
    "Consider a simple gridworld where actions do not result in deterministic state changes. We specify that there is a $20\\%$ probability that the selected action would result in a stochastic state transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d4db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic environment\n",
    "env = gridworld(wind_p=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7df4d",
   "metadata": {},
   "source": [
    "The following set of commands will help you familiarize with different components of the gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67209706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Reward For each Tile \n",
      "\n",
      "\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |-5 |5 |\n",
      "----------\n",
      "0 |0 |0 |"
     ]
    }
   ],
   "source": [
    "print('\\n Reward For each Tile \\n')\n",
    "env.print_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe348607",
   "metadata": {},
   "source": [
    "Check out the set of possible actions for the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da25d71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Set of possible actions in numerical form. These are actual inputs to the gridworld agent \n",
      "\n",
      "[0 1 2 3]\n",
      "\n",
      " Set of possible actions in the grid in text form. They map 1 to 1 from numbers above to direction \n",
      "\n",
      "['U' 'L' 'D' 'R']\n"
     ]
    }
   ],
   "source": [
    "print('\\n Set of possible actions in numerical form. These are actual inputs to the gridworld agent \\n')\n",
    "print(env.action_space)\n",
    "\n",
    "print('\\n Set of possible actions in the grid in text form. They map 1 to 1 from numbers above to direction \\n')\n",
    "print(env.action_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13240b",
   "metadata": {},
   "source": [
    "Consider a policy which tries to reach the goal state(+5) as fast as possible. Below we define the policy to evaluate the state values for this policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4333604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Policy: Fastest Path to Goal State(Does not take reward into consideration) \n",
      "\n",
      "\n",
      "----------\n",
      "R |R |D |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy_fast = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 2,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "print('\\n Policy: Fastest Path to Goal State(Does not take reward into consideration) \\n')\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ac3f4",
   "metadata": {},
   "source": [
    "**Q1**\n",
    "\n",
    "Implement the `get_v` and `get_q` methods to estimate the state value and state-action value in `simple_grid_agent.py`. These may be used later on for debugging your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014e037",
   "metadata": {},
   "source": [
    "**Q2** \n",
    "\n",
    "The Monte Carlo rollout itself has been implemented in `simple_grid_agent.py` inside the `run_episode` method.\n",
    "\n",
    "**Implement** \n",
    "\n",
    "First-visit as well as any-visit Monte Carlo state-value estimation equations inside `mc_predict_v` in `simple_grid_agent.py`.\n",
    "These have been discussed in class. Refer to Sutton and Barto Chapter 5 for further details to implement them.\n",
    "\n",
    "Test and report inside this notebook the results using the following commands. Are there sufficient differences in the state values under anyvisit and firstvisit MC Prediction? Why?\n",
    "\n",
    "NB: assume anyvist and everyvisit to be interchangeable terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d66221c",
   "metadata": {},
   "source": [
    "#### From the printed state values from two different approaches, we can tell that: \n",
    "##### 1. There are differences between the values from these two estimation approaches, First-visit approach has a slightly higher estimation than Every-visit approach overall. It might be caused by the change of state value estimation over time. The agent will probabaly change its expecitation/estimation of same states while visiting those states over and over again.\n",
    "##### 2. The trend of the changes between states are similar, which mean both approaches can give similar guidance/advice about what states to reach for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d22e763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " State Values for first_visit MC state estiamtion \n",
      "\n",
      "\n",
      "---------------\n",
      "0.2 |1.8 |3.5 |\n",
      "---------------\n",
      "-2.4 |3.6 |0 |\n",
      "---------------\n",
      "-2.7 |-2.1 |3.0 |\n",
      " State Values for any_visit MC state estiamtion \n",
      "\n",
      "\n",
      "---------------\n",
      "-1.0 |0.7 |2.7 |\n",
      "---------------\n",
      "-3.6 |2.0 |0 |\n",
      "---------------\n",
      "-4.0 |-3.4 |2.4 |"
     ]
    }
   ],
   "source": [
    " # evaluate state values for policy_fast for both first-vist and any-vist\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "print('\\n State Values for first_visit MC state estiamtion \\n')\n",
    "a.mc_predict_v()\n",
    "a.print_v()\n",
    "\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "print('\\n State Values for any_visit MC state estiamtion \\n')\n",
    "a.mc_predict_v(first_visit=False)\n",
    "a.print_v()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb48cd7",
   "metadata": {},
   "source": [
    "**Q3** \n",
    "\n",
    "The Monte Carlo rollout itself has been implemented in `simple_grid_agent.py` inside the `run_episode` method.\n",
    "\n",
    "**Implement** \n",
    "\n",
    "First-visit as well as any-visit Monte Carlo state-action value estimation equations inside `mc_predict_q` in `simple_grid_agent.py`\n",
    "These have been discussed in class. Refer to Sutton and Barto Chapter 5 for further details to implement them.\n",
    "\n",
    "Test and report inside this notebook the results using the following commands. Are there sufficient differences in the state values under anyvisit and firstvisit MC Q value Prediction? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc5ac2",
   "metadata": {},
   "source": [
    "#### From the printed information from two different approaches, we can tell\n",
    "##### 1. There are differences between the (state,action) values from two approaches, because we are trying to do a sequential control, the (state,action) tuple might be visited for multiple times from different(or even the same) trajectory generated. So there are changes between the (state,action) value estimation, since the position of the tuple, the times of visits of the tuple might matter.\n",
    "##### 2. However, we can tell that both approaches give the similar answers. And this means, no matter First-visit or Every-visit can provide guidance to the agent to make optimal moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14bc2881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " State action Values for first_visit MC state action estiamtion \n",
      "\n",
      "\n",
      " Actions ['U' 'L' 'D' 'R'] \n",
      "\n",
      "(2, 1) [-2.73734084 -3.9595396  -3.42849716  1.38691585]\n",
      "(2, 0) [-3.66056765 -3.91003537 -3.88193042 -3.02787543]\n",
      "(2, 2) [ 3.45119264 -3.29143713  1.43453296  1.23892577]\n",
      "(1, 1) [-0.27670621 -3.43372339 -2.97218811  3.74845041]\n",
      "(0, 1) [-0.06783709 -1.58338738 -3.51127176  1.92980106]\n",
      "(0, 0) [-1.22502463 -1.46422928 -3.97160695  0.017357  ]\n",
      "(1, 0) [-1.50851959 -3.70565529 -4.11510011 -2.94032953]\n",
      "(0, 2) [1.69589533 0.10174607 3.44474671 1.85637429]\n",
      "(1, 2) [0. 0. 0. 0.]\n",
      "\n",
      " State action Values for any_visit MC state action estiamtion \n",
      "\n",
      "\n",
      " Actions ['U' 'L' 'D' 'R'] \n",
      "\n",
      "(2, 2) [ 3.28081506 -3.53646088  1.3112994   1.24599079]\n",
      "(2, 1) [-3.65257051 -4.55715791 -4.1026761   1.00795125]\n",
      "(2, 0) [-4.21606652 -4.64931366 -4.61675245 -3.83922418]\n",
      "(1, 1) [-0.44290205 -4.35385651 -4.0343107   3.30138878]\n",
      "(1, 0) [-2.28536659 -4.61978074 -4.6861435  -3.66622055]\n",
      "(0, 0) [-1.53592134 -2.27869133 -4.27083841 -0.64674432]\n",
      "(0, 2) [ 1.38051141 -0.33543879  3.29870282  1.59543675]\n",
      "(0, 1) [ 0.15383885 -2.12802301 -3.531755    1.54599995]\n",
      "(1, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# evaluate state action values for policy_fast\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "print('\\n State action Values for first_visit MC state action estiamtion \\n')\n",
    "a.mc_predict_q()\n",
    "# a.print_policy()\n",
    "print('\\n Actions', env.action_text, '\\n')\n",
    "for i in a.q: print(i,a.q[i])\n",
    "    \n",
    "\n",
    "# evaluate state action values for policy_fast\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "print('\\n State action Values for any_visit MC state action estiamtion \\n')\n",
    "a.mc_predict_q(first_visit=False)\n",
    "# a.print_policy()\n",
    "print('\\n Actions', env.action_text, '\\n')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0166ff",
   "metadata": {},
   "source": [
    "**Q4**\n",
    "\n",
    "Now we implement Monte Carlo control using state-action values. \n",
    "\n",
    "**Implement**\n",
    "\n",
    "Complete the snippet in `mc_control_q` inside `simple_grid_agent.py`\n",
    "\n",
    "Test and report inside this notebook the results using the following commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf757e1d",
   "metadata": {},
   "source": [
    "#### From the previous policy, the action for (1, 0) state changed, which makes sense because moving to the right state will give the highest penalty(-5) and only by moving up(or down) can avoid reaching that state. By taking the those suggested actions, the agent would easily reach the \"best\" state, however it is a stochastic envrionment, our agent would still take some undesired actions. Though undesired actions might be taken, the agent will probably still be able to find the optimal action using current policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0cd6d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "R |R |D |\n",
      "----------\n",
      "U |R |U |\n",
      "----------\n",
      "R |R |U |\n",
      " Actions: {env.action_text} \n",
      "\n",
      "(1, 1) [-1.53514061 -4.14393046 -3.845456    3.36764789]\n",
      "(0, 1) [-0.5897438  -2.685826   -4.91362876  1.37192667]\n",
      "(0, 2) [ 0.98782491 -1.6768405   3.31530592  0.95977406]\n",
      "(1, 0) [-0.88901848 -4.67981841 -4.53371289 -3.95512805]\n",
      "(2, 0) [-4.1761788  -4.51790078 -4.47265472 -3.77435347]\n",
      "(2, 1) [-3.68603312 -3.75119676 -4.07400469  1.12058132]\n",
      "(0, 0) [-3.97345906 -1.27387    -5.7291139  -0.44024942]\n",
      "(2, 2) [ 3.56668064 -4.21835055  0.649178    1.10784838]\n",
      "(1, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy_fast = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 2,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "        start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "# Run MC Control\n",
    "a.mc_control_q(n_episode = 1000,first_visit=False)\n",
    "a.print_policy()\n",
    "\n",
    "print('\\n Actions: {env.action_text} \\n')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e25c50",
   "metadata": {},
   "source": [
    "**Q5**\n",
    "\n",
    "Bonus!\n",
    "\n",
    "**Implement**\n",
    "\n",
    "Greedy within The Limit of  Iinfinite Exploration MC Control in `mc_control_glie` function inside `simple_grid_agent.py`\n",
    "\n",
    "Test and report inside this notebook the results using the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "597e3cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "R |U |L |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "U |R |D |\n",
      " Actions ['U' 'L' 'D' 'R'] \n",
      "\n",
      "(1, 1) [-0.01703777 -0.05953831 -0.0296877   0.00555554]\n",
      "(2, 0) [ 0.01084514 -0.03243344 -0.01962902 -0.00332621]\n",
      "(2, 1) [-0.00310368 -0.11587576 -0.0558164   0.01347614]\n",
      "(2, 2) [ 0.0240962  -0.12748024  0.17665254  0.13684205]\n",
      "(1, 0) [-0.18295813 -0.0545739  -0.01946614  0.00767557]\n",
      "(0, 1) [ 0.12241883 -0.23462894 -0.1849119   0.02125264]\n",
      "(0, 0) [-0.12097907 -0.521952   -0.92226336  0.00143244]\n",
      "(0, 2) [-0.15077283  0.02931431  0.02530968 -0.0571634 ]\n",
      "(1, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy_fast = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 2,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "        start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "a.mc_control_glie(n_episode = 1000)\n",
    "a.print_policy()\n",
    "print('\\n Actions', env.action_text, '\\n')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b67ce0",
   "metadata": {},
   "source": [
    "#### Reference:\n",
    "##### https://ai.stackexchange.com/questions/6486/why-is-glie-monte-carlo-control-an-on-policy-control\n",
    "##### https://www.jeremyjordan.me/rl-learning-implementations/\n",
    "\n",
    "#### Notes about my solution:\n",
    "##### 1. new update method for Q values: Q(S, A) <- Q(S, A) + learning_factor * (G - Q(S, A))\n",
    "##### 2. Learning factor is defined as lr if lr is not zero, otherwise it will be assigned by 1/N, N is the number of visits of current state S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5bc54f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
